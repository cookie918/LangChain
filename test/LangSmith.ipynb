{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标提示:\n",
      " input_variables=['input'] template='\\n你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'\n",
      "目标提示:\n",
      " input_variables=['input'] template='\\n你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'\n",
      "路由模板:\n",
      " Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "flower_care: 适合回答关于鲜花护理的问题\n",
      "flower_decoration: 适合回答关于鲜花装饰的问题\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n",
      "路由提示:\n",
      " input_variables=['input'] output_parser=RouterOutputParser() template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nflower_care: 适合回答关于鲜花护理的问题\\nflower_decoration: 适合回答关于鲜花装饰的问题\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\\n'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "flower_care: {'input': '如何正确地给玫瑰浇水以确保它们健康成长？'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n",
      "下面是需要你来回答的问题:\n",
      "如何正确地给玫瑰浇水以确保它们健康成长？\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "浇水玫瑰花有几点需要注意：\n",
      "\n",
      "1. **适量浇水**：玫瑰喜欢湿润但排水良好的土壤。一般来说，每周浇水一次就足够了，但在炎热的夏季或干燥的气候中可能需要更频繁。浇水的量应使水分渗透到根部，但避免让水积聚在植物周围，以免引起根部腐烂。\n",
      "\n",
      "2. **深度浇水**：浇水时尽量深入土壤，而不是只湿润表层。这有助于鼓励玫瑰的根系向下生长，从而更好地吸收水分和营养。\n",
      "\n",
      "3. **早晨浇水**：最佳的浇水时间是早晨，这样叶子有足够的时间在夜晚之前晾干，可以减少疾病的发生。\n",
      "\n",
      "4. **避免水溅到叶子**：尽量不要让水直接喷洒到玫瑰的叶子和花朵上，因为湿润的叶子容易引发霉菌和病害。\n",
      "\n",
      "5. **观察天气和季节**：在雨季或者湿度高的天气，可以减少浇水次数。而在冬季，当植物进入休眠状态时，浇水应减少到足以防止土壤完全干燥的程度。\n",
      "\n",
      "6. **使用浇水壶或灌溉系统**：使用浇水壶可以控制水流，确保水直接到达根部。如果你有一个大型的玫瑰花园，安装一个灌溉系统可能会更方便。\n",
      "\n",
      "7. **考虑土壤类型**：不同类型的土壤保水能力不同，沙质土壤可能需要更频繁的浇水，而粘土土壤则需要更少。\n",
      "\n",
      "记住，每个地区的气候和土壤条件都不同，所以你需要根据你的具体情况调整浇水策略。观察你的玫瑰对水分的反应是最重要的，如果叶子出现黄化或萎缩，可能是浇水过多或过少的迹象。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''欢迎来到LangChain实战课\n",
    "https://time.geekbang.org/column/intro/100617601\n",
    "作者 黄佳'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-5e56b4e6ddba4a6a8110e168d8dd8377\"\n",
    "# 设置OpenAI API密钥\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'Your Key'\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 构建两个场景的模板\n",
    "flower_care_template = \"\"\"\n",
    "你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n",
    "下面是需要你来回答的问题:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "flower_deco_template = \"\"\"\n",
    "你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\n",
    "下面是需要你来回答的问题:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "# 构建提示信息\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"key\": \"flower_care\",\n",
    "        \"description\": \"适合回答关于鲜花护理的问题\",\n",
    "        \"template\": flower_care_template,\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"flower_decoration\",\n",
    "        \"description\": \"适合回答关于鲜花装饰的问题\",\n",
    "        \"template\": flower_deco_template,\n",
    "    }\n",
    "]\n",
    "\n",
    "# 初始化语言模型\n",
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI()\n",
    "llm = Tongyi(deployment_name=\"GPT-4\", temperature=0)\n",
    "\n",
    "# 构建目标链\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_map = {}\n",
    "\n",
    "for info in prompt_infos:\n",
    "    prompt = PromptTemplate(\n",
    "        template=info['template'],\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "    print(\"目标提示:\\n\", prompt)\n",
    "    \n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    chain_map[info[\"key\"]] = chain\n",
    "\n",
    "# 构建路由链\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE as RounterTemplate\n",
    "\n",
    "destinations = [f\"{p['key']}: {p['description']}\" for p in prompt_infos]\n",
    "router_template = RounterTemplate.format(destinations=\"\\n\".join(destinations))\n",
    "print(\"路由模板:\\n\", router_template)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "print(\"路由提示:\\n\", router_prompt)\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm,\n",
    "    router_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 构建默认链\n",
    "from langchain.chains import ConversationChain\n",
    "default_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    output_key=\"text\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 构建多提示链\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=chain_map,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试1\n",
    "print(chain.run(\"如何为玫瑰浇水？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This morning's meeting was remarkable, as we discussed and successfully resolved all world conflicts.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'ls__eaf357058c534c5da51bd8574575c046'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'default'\n",
    "\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-5e56b4e6ddba4a6a8110e168d8dd8377\"\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "])\n",
    "model = Tongyi()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! How can I help you today?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'ls__eaf357058c534c5da51bd8574575c046'\n",
    "\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'langchain_for_llm_application_development'\n",
    "\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-5e56b4e6ddba4a6a8110e168d8dd8377\"\n",
    " \n",
    "from langchain_community.llms import Tongyi\n",
    "llm = Tongyi()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
